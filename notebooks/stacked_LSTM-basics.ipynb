{"cells":[{"cell_type":"markdown","id":"e5507c43-3829-4975-a889-bc786711177a","metadata":{},"source":["Estimated time needed: **15** minutes\n"]},{"cell_type":"markdown","id":"3a5bbc07-6999-48a6-8af9-ab117190de93","metadata":{},"source":["<a id=\"instructions\"><a/> \n","\n","<h2>LSTM and stacked LSTM</h2>\n","    \n","We start by installing everything we need for this exercise:\n"]},{"cell_type":"code","execution_count":null,"id":"7cc206f1-f179-4a56-aad8-d0f032a4869e","metadata":{},"outputs":[],"source":["#!pip install grpcio==1.24.3\n","#!pip install tensorflow==2.9.0"]},{"cell_type":"markdown","id":"3f028e0d-7827-410f-a6ac-86c4a33faf7d","metadata":{},"source":["<a id=\"ltsm\"></a>\n","\n","<h2>LSTM</h2>\n","Lets first create a tiny LSTM network sample to understand the architecture of LSTM networks.\n"]},{"cell_type":"markdown","id":"25051d8b-e1c0-4471-9fa8-3de596d35e03","metadata":{},"source":["We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow.keras.layers</code></b> , which includes the function for building RNNs.\n"]},{"cell_type":"code","execution_count":2,"id":"3aa41070-23bf-48ff-96b5-acb1a905e2e4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.17.0\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","if not tf.__version__ == '2.9.0':\n","    print(tf.__version__)\n","    #raise ValueError('please upgrade to TensorFlow 2.9.0, or restart your Kernel (Kernel->Restart & Clear Output)')\n"]},{"cell_type":"markdown","id":"4161c0d0-5e1f-4f6d-91c4-cdc8b29c0c92","metadata":{},"source":["IMPORTANT! => Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Outout\" and wait until all output disapears. Then your changes are beeing picked up\n"]},{"cell_type":"markdown","id":"dc129658-eaba-4165-a2f6-8653e64da609","metadata":{},"source":["We want to create a network that has only one LSTM cell. We have to pass 2 elements to LSTM, the <b>prv_output</b> and <b>prv_state</b>, so called, <b>h</b> and <b>c</b>. Therefore, we initialize a state vector, <b>state</b>.  Here, <b>state</b> is a tuple with 2 elements, each one is of size [1 x 4], one for passing prv_output to next time step, and another for passing the prv_state to next time stamp.\n"]},{"cell_type":"code","execution_count":3,"id":"a7ba6143-0076-4475-b26b-d85f6f6c44fd","metadata":{},"outputs":[{"data":{"text/plain":["(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>,\n"," <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["LSTM_CELL_SIZE = 4  # output size (dimension), which is same as hidden size in the cell\n","\n","state = (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n","state"]},{"cell_type":"code","execution_count":4,"id":"66e4c54c-a45e-4d6e-a697-5ce32b3660ae","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n"]}],"source":["lstm = tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)\n","\n","lstm.states=state\n","\n","print(lstm.states)\n"]},{"cell_type":"markdown","id":"a69c2705-cd98-4c4e-8ffc-35a6ef291739","metadata":{},"source":["As we can see, the states has 2 parts, the new state c, and also the output h. Lets check the output again:\n"]},{"cell_type":"markdown","id":"18a80585-9aba-4f03-a058-a0a91f835587","metadata":{},"source":["Let define a sample input. In this example, batch_size = 1, and  features = 6:\n"]},{"cell_type":"code","execution_count":5,"id":"99f9b8d7-f463-4417-a236-f510ca402514","metadata":{},"outputs":[],"source":["#Batch size x time steps x features.\n","sample_input = tf.constant([[3,2,2,2,2,2]],dtype=tf.float32)\n","\n","batch_size = 1\n","sentence_max_length = 1\n","n_features = 6\n","\n","new_shape = (batch_size, sentence_max_length, n_features)\n","\n","inputs = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"]},{"cell_type":"markdown","id":"d41481a1-35d6-4a73-bc8e-2abbde30c85d","metadata":{},"source":["Now, we can pass the input to lstm_cell, and check the new state:\n"]},{"cell_type":"code","execution_count":6,"id":"0717e807-a0c1-418f-9a87-15fce806c192","metadata":{},"outputs":[],"source":["output, final_memory_state, final_carry_state = lstm(inputs)\n"]},{"cell_type":"code","execution_count":7,"id":"912da523-fa22-448d-b0d0-91e2bf4cf13f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output :  tf.Tensor([1 1 4], shape=(3,), dtype=int32)\n","Memory :  tf.Tensor([1 4], shape=(2,), dtype=int32)\n","Carry state :  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"]}],"source":["print('Output : ', tf.shape(output))\n","\n","print('Memory : ',tf.shape(final_memory_state))\n","\n","print('Carry state : ',tf.shape(final_carry_state))"]},{"cell_type":"markdown","id":"af75a60d-9220-46c1-8f46-0624a4a270e8","metadata":{},"source":["<hr>\n","<a id=\"stacked_ltsm\"></a>\n","<h2>Stacked LSTM</h2>\n","What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second.\n"]},{"cell_type":"markdown","id":"1c2d2bc5-32d5-4f7e-93d2-b3a72af75fc1","metadata":{},"source":["Lets create the stacked LSTM cell:\n"]},{"cell_type":"code","execution_count":8,"id":"493e4645-ef30-4ef6-8203-894e76cc3ea7","metadata":{},"outputs":[],"source":["cells = []"]},{"cell_type":"markdown","id":"11652ce4-f19b-4534-b8bf-83d869f3aa91","metadata":{},"source":["Creating the first layer LTSM cell.\n"]},{"cell_type":"code","execution_count":9,"id":"8cd82c00-97cd-4c31-a490-7b91fcc20c8d","metadata":{},"outputs":[],"source":["LSTM_CELL_SIZE_1 = 4 #4 hidden nodes\n","cell1 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_1)\n","cells.append(cell1)"]},{"cell_type":"markdown","id":"a8415887-ce9d-4de2-bf6e-4c00c7f48306","metadata":{},"source":["Creating the second layer LTSM cell.\n"]},{"cell_type":"code","execution_count":10,"id":"c66bf07d-a695-47b2-aef3-908f28f0062b","metadata":{},"outputs":[],"source":["LSTM_CELL_SIZE_2 = 5 #5 hidden nodes\n","cell2 = tf.keras.layers.LSTMCell(LSTM_CELL_SIZE_2)\n","cells.append(cell2)"]},{"cell_type":"markdown","id":"2ccfbc40-d83d-4f12-afeb-cefa97643bf0","metadata":{},"source":["To create a multi-layer LTSM we use the <b>tf.keras.layers.StackedRNNCells</b> function, it takes in multiple single layer LTSM cells to create a multilayer stacked LTSM model.\n"]},{"cell_type":"code","execution_count":11,"id":"913bf09a-e5c6-4f08-a7aa-81e7977f2157","metadata":{},"outputs":[],"source":["stacked_lstm =  tf.keras.layers.StackedRNNCells(cells)"]},{"cell_type":"markdown","id":"eb240834-656e-4c6c-9d06-3f9d77ca70ec","metadata":{},"source":["Now we can create the RNN from <b>stacked_lstm</b>:\n"]},{"cell_type":"code","execution_count":12,"id":"e3155965-dd96-4342-9c9e-a27cf9b5d461","metadata":{},"outputs":[],"source":["lstm_layer= tf.keras.layers.RNN(stacked_lstm ,return_sequences=True, return_state=True)"]},{"cell_type":"markdown","id":"da5784f5-0c06-45dc-85fa-e73f6f3ddeec","metadata":{},"source":["Lets say the input sequence length is 3, and the dimensionality of the inputs is 6. The input should be a Tensor of shape: [batch_size, max_time, dimension], in our case it would be (2, 3, 6)\n"]},{"cell_type":"code","execution_count":13,"id":"b6b4afd6-f61e-406b-9133-4f10c05c7a66","metadata":{},"outputs":[],"source":["#Batch size x time steps x features.\n","sample_input = [[[1,2,3,4,3,2], [1,2,1,1,1,2],[1,2,2,2,2,2]],[[1,2,3,4,3,2],[3,2,2,1,1,2],[0,0,0,0,3,2]]]\n","sample_input\n","\n","batch_size = 2\n","time_steps = 3\n","features = 6\n","new_shape = (batch_size, time_steps, features)\n","\n","x = tf.constant(np.reshape(sample_input, new_shape), dtype = tf.float32)"]},{"cell_type":"markdown","id":"d1c50edd-cc1d-4233-8cc2-751b9a949b39","metadata":{},"source":["we can now send our input to network, and check the output:\n"]},{"cell_type":"code","execution_count":14,"id":"837d3c58-2f8c-4bc4-a995-59a571968747","metadata":{},"outputs":[],"source":["output, final_memory_state, final_carry_state  = lstm_layer(x)"]},{"cell_type":"code","execution_count":15,"id":"e825f252-2588-455b-a342-cd67eecd9bb4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output :  tf.Tensor([2 3 5], shape=(3,), dtype=int32)\n","Memory :  tf.Tensor([2 2 4], shape=(3,), dtype=int32)\n","Carry state :  tf.Tensor([2 2 5], shape=(3,), dtype=int32)\n"]}],"source":["print('Output : ', tf.shape(output))\n","\n","print('Memory : ',tf.shape(final_memory_state))\n","\n","print('Carry state : ',tf.shape(final_carry_state))"]},{"cell_type":"markdown","id":"e32e3f71-dfac-454f-ac65-63a829f9eed0","metadata":{},"source":["As you see, the output is of shape (2, 3, 5), which corresponds to our 2 batches, 3 elements in our sequence, and the dimensionality of the output which is 5.\n","\n","<hr>\n"]}],"metadata":{"kernelspec":{"display_name":"gpflow","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"prev_pub_hash":"7b8d0a99eb8bfffc400260c54fe7714038957760ea4267ffa666397e1561f696"},"nbformat":4,"nbformat_minor":4}
